# -*- coding: utf-8 -*-
"""Enhanced_XLM_R_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FA3JwHAPOfgX73N4xdXSwI-giLFDMG1A
"""

!pip install -q transformers datasets scikit-learn pandas matplotlib seaborn torch
!pip install imbalanced-learn

"""Importing libraries"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from collections import Counter, defaultdict
import random
import re
from sklearn.metrics import accuracy_score
import numpy as np
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
# AdamW is no longer imported directly but is now used through TrainingArguments
#from transformers.optimization import AdamW
from torch.nn import CrossEntropyLoss
import joblib
from transformers import AutoTokenizer
from transformers import Trainer, TrainingArguments
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

"""Load and Inspect Your Dataset"""

import pandas as pd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/gdrive')

# Define file path (Update the filename to match your uploaded CSV file)
file_path = "/content/gdrive/My Drive/Colab Notebooks/eduu.csv"  # Make sure the filename is correct

# Load dataset
df = pd.read_csv(file_path)

# Preview the dataset
print(f"Dataset shape: {df.shape}")
df.head()

"""Check for missing values and column names"""

print("Columns:", df.columns)
print("\nMissing values:\n", df.isnull().sum())

"""Encode Labels and Split the Dataset"""

label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['Thematic Area'])
label_names = label_encoder.classes_
num_labels = len(label_names)

print(f"Number of classes: {num_labels}")
print("Label mapping:")
for i, label in enumerate(label_names):
    print(f"{i}: {label}")

"""Split the dataset (70/10/20)"""

train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(
    df['Swahili Phrase'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    stratify=df['label'],
    random_state=42
)

train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_val_texts,
    train_val_labels,
    test_size=0.125,
    stratify=train_val_labels,
    random_state=42
)

print(f"Train size: {len(train_texts)}")
print(f"Validation size: {len(val_texts)}")
print(f"Test size: {len(test_texts)}")

"""Text-Level Augmentation Functions"""

swahili_stopwords = set(["na", "ya", "wa", "kwa", "ni", "hii", "katika", "kama", "hivyo", "au", "mara", "pia", "za", "hapo"])
swahili_synonyms = {
    "shamba": ["ardhi", "bustani"],
    "bidii": ["nguvu", "juhudi"],
    "walilima": ["walitumia", "walifanya kazi"],
    "kazi": ["juhudi", "ajira"]
}

def tokenize(text):
    return re.findall(r'\b\w+\b', text.lower())

def synonym_replacement(words, n=1):
    new_words = words.copy()
    candidates = [w for w in words if w not in swahili_stopwords and w in swahili_synonyms]
    random.shuffle(candidates)
    for i in range(min(n, len(candidates))):
        word = candidates[i]
        synonym = random.choice(swahili_synonyms[word])
        new_words = [synonym if w == word else w for w in new_words]
    return new_words

def random_deletion(words, p=0.1):
    if len(words) == 1:
        return words
    return [word for word in words if random.uniform(0,1) > p]

def random_swap(words, n=1):
    new_words = words.copy()
    for _ in range(n):
        idx1, idx2 = random.sample(range(len(new_words)), 2)
        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]
    return new_words

def random_insertion(words, n=1):
    new_words = words.copy()
    for _ in range(n):
        word = random.choice(words)
        if word in swahili_synonyms:
            synonym = random.choice(swahili_synonyms[word])
            insert_pos = random.randint(0, len(new_words))
            new_words.insert(insert_pos, synonym)
    return new_words

def eda(text, num_aug=1):
    words = tokenize(text)
    if not words:
        return [text]
    augmented_texts = []
    for _ in range(num_aug):
        method = random.choice(["synonym", "deletion", "swap", "insertion"])
        if method == "synonym":
            aug = synonym_replacement(words, n=2)
        elif method == "deletion":
            aug = random_deletion(words, p=0.1)
        elif method == "swap":
            aug = random_swap(words, n=2)
        elif method == "insertion":
            aug = random_insertion(words, n=2)
        augmented_texts.append(" ".join(aug))
    return augmented_texts

"""Augment Only Minority Classes"""

from collections import defaultdict
import random

label_to_texts = defaultdict(list)
for text, label in zip(train_texts, train_labels):
    label_to_texts[label].append(text)

max_count = max(len(texts) for texts in label_to_texts.values())

augmented_texts = []
augmented_labels = []

for label, texts in label_to_texts.items():
    current_count = len(texts)
    needed = max_count - current_count
    if needed <= 0:
        continue

    print(f"Augmenting class {label} with {needed} samples...")

    attempts = 0
    generated = 0
    max_attempts = needed * 5  # Prevent infinite loops

    while generated < needed and attempts < max_attempts:
        text = random.choice(texts)
        attempts += 1

        try:
            if not isinstance(text, str):
                text = str(text) if not pd.isna(text) else ""
            if len(text.split()) < 4:
                continue

            aug_samples = eda(text, num_aug=min(3, needed - generated))
            augmented_texts.extend(aug_samples)
            augmented_labels.extend([label] * len(aug_samples))
            generated += len(aug_samples)

        except Exception as e:
            print(f"Skipping: '{text}' | Error: {e}")

"""Add augmented samples to training set"""

train_texts += augmented_texts
train_labels += augmented_labels

print("\n✅ Augmentation complete. New training size:", len(train_texts))

"""Load the tokenizer"""

tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

from torch.utils.data import Dataset

class SwahiliTextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])
        encoding = self.tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
        # Squeeze to remove the extra batch dimension
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'label': torch.tensor(label, dtype=torch.long)
        }

"""Create dataset objects"""

train_dataset = SwahiliTextDataset(train_texts, train_labels, tokenizer)
val_dataset   = SwahiliTextDataset(val_texts, val_labels, tokenizer)
test_dataset  = SwahiliTextDataset(test_texts, test_labels, tokenizer)

# Optional: create DataLoaders
batch_size = 16

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=batch_size)
test_loader  = DataLoader(test_dataset, batch_size=batch_size)

"""Load the Pretrained Model with a Classification Head (12 labels)"""

from transformers import XLMRobertaForSequenceClassification

# Load the pretrained XLM-R model with a classification head for 12 labels
model = XLMRobertaForSequenceClassification.from_pretrained(
    "xlm-roberta-base",
    num_labels=12  # You are classifying into 12 thematic categories
)

"""Move Model to GPU (if available)"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""Set Up the Optimizer (AdamW)"""

from torch.optim import AdamW  # ✅ Use this instead

optimizer = AdamW(model.parameters(), lr=2e-5)

"""Set the Loss Function"""

criterion = CrossEntropyLoss()

"""Define the Training and Evaluation Loops"""

def train(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    all_preds, all_labels = [], []

    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        logits = outputs.logits

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.detach().cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    acc = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)

    return avg_loss, acc, precision, recall, f1

"""Evaluation loop"""

from sklearn.metrics import classification_report

def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    # Compute metrics using classification_report
    report = classification_report(all_labels, all_preds, output_dict=True)

    # Extract relevant metrics
    acc = report['accuracy']
    precision = report['macro avg']['precision']
    recall = report['macro avg']['recall']
    f1 = report['macro avg']['f1-score']

    avg_loss = total_loss / len(dataloader)

    return avg_loss, acc, precision, recall, f1

"""Run Training for N Epochs"""

num_epochs = 5  # Set the number of epochs

for epoch in range(num_epochs):
    print(f"\nEpoch {epoch+1}/{num_epochs}")

    train_loss, train_acc, train_prec, train_rec, train_f1 = train(model, train_loader, optimizer, criterion, device)
    val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate(model, val_loader, criterion, device)

    print(f"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Precision: {train_prec:.4f} | Recall: {train_rec:.4f} | F1: {train_f1:.4f}")
    print(f"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Precision: {val_prec:.4f} | Recall: {val_rec:.4f} | F1: {val_f1:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Collect predictions on the validation set
model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1).cpu().numpy()

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

# Provide full class names matching the number of labels
class_names = [
    "Agriculture and Food", "Automotive and Transport", "Business and Finance", "Ceremony",
     "Education and Technology", "Healthcare", "History and Government",
    "Nature and Environment", "News and Media", "Religion and Culture", "Social Setting", "Sports and Entertainment"
]

# Generate report and confusion matrix
print(classification_report(all_labels, all_preds, target_names=class_names))

cm = confusion_matrix(all_labels, all_preds)

# Plot
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Enhanced Confusion Matrix")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1).cpu().numpy()

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

""" Saving the Model and Tokenizer (Hugging Face format)"""

from transformers import AutoTokenizer

# Directory to save model and tokenizer
save_directory = "swahili_topic_classifier_model"

# Save model and tokenizer
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

import shutil

shutil.make_archive(save_directory, 'zip', save_directory)

pip install transformers huggingface_hub

from huggingface_hub import login

login()  # You’ll be prompted to enter your HF token

model.push_to_hub("Edwintos/XLM-R_Base_Swahili_Topic_Classification_Model")
tokenizer.push_to_hub("Edwintos/XLM-R_Base_Swahili_Topic_Classification_Model")