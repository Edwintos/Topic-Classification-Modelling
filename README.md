This repository contains scripts and configurations for fine-tuning the XLM-R model, a transformer-based multilingual language model, for Swahili short-text topic classification. The fine-tuning process adapts XLM-R to classify texts into twelve thematic categories using the Swahili-Dholuo Topic Classification (SDTC) dataset, which consists of Swahili and Dholuo text pairs stored in an XLSX file. Before running the fine-tuning script, ensure you have Python 3.8+, PyTorch, Transformers (Hugging Face), Datasets (Hugging Face), scikit-learn, pandas, tqdm, openpyxl (for reading Excel files), and CUDA (if training on a GPU). Install dependencies using pip install torch transformers datasets scikit-learn pandas tqdm openpyxl and verify GPU availability with python -c "import torch; print(torch.cuda.is_available())". The dataset is structured with three columns: Dholuo Phrase, Swahili Phrase, and Thematic Area, and should be loaded as a Pandas DataFrame before processing. Fine-tuning is executed using python fine_tune_xlmr.py --train_data path/to/dataset.xlsx --test_data path/to/test_dataset.xlsx --epochs 8 --batch_size 16 --learning_rate 3e-5 --output_dir saved_model/, with hyperparameters such as the number of epochs, batch size, learning rate, and sequence length customizable as needed. Model performance is evaluated using accuracy (0.6239), precision (0.6150), recall (0.6239), and F1-score (0.6141). The fine-tuned model weights are stored in saved_model/, logs and performance metrics in logs/, and predictions on test data in predictions.csv. To use the trained model for inference, a script is provided for tokenizing input text and classifying it into a thematic category. Common troubleshooting issues include reducing batch size for memory constraints, ensuring the dataset is properly formatted, and fine-tuning hyperparameters to improve performance. This project builds on the XLM-R model and the Hugging Face Transformers library and is released under the Creative Commons BY 4.0 license.
