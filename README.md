<<<<<<< HEAD
# XLM-R
=======
This repository contains scripts and configurations for fine-tuning the XLM-R model, a transformer-based multilingual language model, for Swahili short-text topic classification. The fine-tuning process adapts XLM-R to classify texts into twelve thematic categories using the Swahili-Dholuo Topic Classification (SDTC) dataset, which consists of Swahili and Dholuo text pairs stored in an XLSX file. Before running the fine-tuning script, ensure you have Python 3.8+, PyTorch, Transformers (Hugging Face), Datasets (Hugging Face), scikit-learn, pandas, tqdm, openpyxl (for reading Excel files), and CUDA (if training on a GPU). Install dependencies using pip install torch transformers datasets scikit-learn pandas tqdm openpyxl and verify GPU availability with python -c "import torch; print(torch.cuda.is_available())". The dataset is structured with three columns: Dholuo Phrase, Swahili Phrase, and Thematic Area, and should be loaded as a Pandas DataFrame before processing. Fine-tuning is executed using python fine_tune_xlmr.py --train_data path/to/dataset.xlsx --test_data path/to/test_dataset.xlsx --epochs 8 --batch_size 16 --learning_rate 3e-5 --output_dir saved_model/, with hyperparameters such as the number of epochs, batch size, learning rate, and sequence length customizable as needed. Model performance is evaluated using accuracy (0.6239), precision (0.6150), recall (0.6239), and F1-score (0.6141). The fine-tuned model weights are stored in saved_model/, logs and performance metrics in logs/, and predictions on test data in predictions.csv. To use the trained model for inference, a script is provided for tokenizing input text and classifying it into a thematic category. Common troubleshooting issues include reducing batch size for memory constraints, ensuring the dataset is properly formatted, and fine-tuning hyperparameters to improve performance. This project builds on the XLM-R model and the Hugging Face Transformers library and is released under the Creative Commons BY 4.0 license.

<<<<<<< HEAD
# Naive Bayes
=======
This repository contains scripts and configurations for training a Naive Bayes model for Swahili short-text topic classification. The model is trained to classify texts into twelve thematic categories using the Swahili-Dholuo Topic Classification (SDTC) dataset, which consists of paired Swahili and Dholuo phrases. To get started, ensure that you have Python 3.8+ and the required dependencies installed, such as scikit-learn, pandas, and tqdm, by running pip install scikit-learn pandas tqdm. The dataset must be loaded as a Pandas DataFrame, and it is structured with three columns: "Dholuo Phrase," "Swahili Phrase," and "Thematic Area." You can start the training process by executing the train_naive_bayes.py script with the appropriate paths to the training and test datasets, as well as an optional output directory for saving the trained model. The script also provides customizable hyperparameters for the Naive Bayes classifier, such as the smoothing parameter.
After training, the performance of the model is evaluated using standard classification metrics. The Naive Bayes model yielded a test accuracy of 0.5372, a precision score of 0.6074, a recall of 0.5372, and an F1 score of 0.4425. The trained model is saved in the specified output directory, and predictions are available for new inputs. A separate script for inference allows for classifying new Swahili text into the twelve thematic categories. Common troubleshooting includes ensuring the dataset is correctly formatted and adjusting hyperparameters to optimize performance. This project uses scikit-learn for training and evaluation and is released under the Creative Commons BY 4.0 license.

<<<<<<< HEAD
# Bi-LSTM
=======
This repository provides scripts and configurations for training a BiLSTM (Bidirectional Long Short-Term Memory) model for Swahili short-text topic classification. The BiLSTM model is trained to categorize texts into twelve thematic categories using the Swahili-Dholuo Topic Classification (SDTC) dataset. To run the training, you need Python 3.8+ and dependencies such as PyTorch, scikit-learn, pandas, and tqdm. You can install these using the command pip install torch scikit-learn pandas tqdm. The dataset is structured with columns for "Dholuo Phrase," "Swahili Phrase," and "Thematic Area," and should be loaded as a Pandas DataFrame before being passed to the model. The training process is initiated by running the train_bilstm.py script, specifying the paths to the training and test datasets, as well as optional hyperparameters for training, including the number of epochs, batch size, and learning rate.
Upon completion, the model's performance is evaluated using metrics like accuracy, precision, recall, and F1-score. The BiLSTM model achieved a test accuracy of 0.5857, precision of 0.5762, recall of 0.5857, and an F1 score of 0.5770. These results demonstrate a moderate level of performance for the BiLSTM model. The trained model is saved in the output directory, and predictions for new Swahili text can be made using an inference script. If training performance is suboptimal, adjusting hyperparameters or using a smaller batch size might help. The project uses PyTorch for model training and evaluation and is released under the Creative Commons BY 4.0 license.

